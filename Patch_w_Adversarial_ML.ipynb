{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Patch w/ Adversarial ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4f+ir+/Uest0GTYBjZg/s"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-uOxwBQUhhi"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "# IMPORT PATCH NET\n",
        "\n",
        "\n",
        "\n",
        "# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n",
        "#       see, https://github.com/pytorch/vision/issues/3497 for more information\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3fa66oS3jNl"
      },
      "source": [
        "epsilons = [] #[0, .05, .1, .15, .2, .25, .3, .35, .4] #, -.5, -.6, -.7, -.8, -.9, -1]\n",
        "#pretrained_model = \"/content/lenet_mnist_model.pth\"\n",
        "use_cuda=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhuM1UiUV7-E"
      },
      "source": [
        "#from PIL import Image\n",
        "from matplotlib import cm\n",
        "\n",
        "def CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n",
        "    # assert 0 <= v <= 20\n",
        "    if v < 0:\n",
        "        return img\n",
        "    #w, h = img.size\n",
        "    w = 28\n",
        "    h = 28\n",
        "    x0 = np.random.uniform(w)\n",
        "    y0 = np.random.uniform(h)\n",
        "\n",
        "    #print(type(img))\n",
        "    #print(np.shape(img))\n",
        "    #plt.imshow(img, cmap=\"gray\") # before patch is added\n",
        "\n",
        "\n",
        "    x0 = int(max(0, x0 - v / 2.))\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = min(w, x0 + v)\n",
        "    y1 = min(h, y0 + v)\n",
        "    \n",
        "    for i in range(len(img)):\n",
        "      if (i >= x0 and i <= x1):\n",
        "        for j in range(len(img[i])):\n",
        "          if (j >= y0 and j <= y1):\n",
        "            img[i][j] = 0.5 #128 #(128, 128, 128)    \n",
        "\n",
        "    #plt.imshow(img, cmap=\"gray\") # after patch is added\n",
        "    \n",
        "    #xy = (x0, y0, x1, y1)\n",
        "    #color = (128, 128, 128) # gray\n",
        "    # color = (0, 0, 0)\n",
        "    #img = img.copy()\n",
        "    #im = Image.fromarray(np.uint8(cm.gist_earth(img)*255))\n",
        "    #PIL.ImageDraw.Draw(im).rectangle(xy, color)\n",
        "    #img = np.array(im)\n",
        "    #plt.imshow(img, cmap=\"gray\")\n",
        "    #print(np.shape(img))\n",
        "    return img, x0, y0, x1, y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvE0RHgunGvi"
      },
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    print('in fgsm. image shape:',np.shape(image))\n",
        "    print('')\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    sign_data_grad = torch.squeeze(sign_data_grad)\n",
        "    print('in fgsm. epsilon*sign_data_grad shape:', np.shape(epsilon*sign_data_grad))\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUY9H-ES4UGI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "d54f707b-03f1-418b-e4b1-c56adb514965"
      },
      "source": [
        "#from PIL import Image\n",
        "'''from matplotlib import cm\n",
        "from PIL import Image, ImageOps\n",
        "def CutoutAbsWithPerturbation(img, v, pos_epsilon_for_attack, data_grad):  # [0, 60] => percentage: [0, 0.2]\n",
        "    # assert 0 <= v <= 20\n",
        "    print('data_grad:', np.shape(data_grad))\n",
        "    \n",
        "    if v < 0:\n",
        "        return img\n",
        "    #w, h = img.size\n",
        "    w = 28\n",
        "    h = 28\n",
        "    x0 = np.random.uniform(w)\n",
        "    y0 = np.random.uniform(h)\n",
        "\n",
        "    #print(type(img))\n",
        "    print(np.shape(img))\n",
        "    #plt.imshow(img, cmap=\"gray\") # before patch is added\n",
        " \n",
        "    gray_img = Image.new('RGB', (5, 5), color = 'gray')\n",
        "    grayscale_img = ImageOps.grayscale(gray_img)\n",
        "    patch_img_no_perturbation = np.array(grayscale_img)\n",
        "    print('patch_img_no_perturbation shape:', np.shape(patch_img_no_perturbation))\n",
        "    #patch_img_no_perturbation = [[0]*5 for i in range(5)] #[[]] # 5 by 5 gray image\n",
        "    #for i in range(v):\n",
        "      #for j in range(v):\n",
        "        #patch_img_no_perturbation[i][j] = 0.5 # gray color is b/t 0 and 1.\n",
        "    \n",
        "\n",
        "    x0 = int(max(0, x0 - v / 2.)) # x0, y0 becomes bottom left\n",
        "    y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = min(w, x0 + v) # x1, y1 becomes bottom right\n",
        "    y1 = min(h, y0 + v)\n",
        "\n",
        "    patch_img_no_perturbation_torch = torch.from_numpy(patch_img_no_perturbation)\n",
        "    \n",
        "    perturbed_patch_img = fgsm_attack(patch_img_no_perturbation_torch, pos_epsilon_for_attack, data_grad)\n",
        "    \n",
        "    for i in range(len(img)):\n",
        "      if (i >= x0 and i <= x1):\n",
        "        for j in range(len(img[i])):\n",
        "          if (j >= y0 and j <= y1):\n",
        "            img[i][j] = .5 #perturbed_patch_img[i-x0][j-y0] #128 #(128, 128, 128) \n",
        "\n",
        "    #plt.imshow(img, cmap=\"gray\") # after patch is added\n",
        "    \n",
        "    #xy = (x0, y0, x1, y1)\n",
        "    #color = (128, 128, 128) # gray\n",
        "    # color = (0, 0, 0)\n",
        "    #img = img.copy()\n",
        "    #im = Image.fromarray(np.uint8(cm.gist_earth(img)*255))\n",
        "    #PIL.ImageDraw.Draw(im).rectangle(xy, color)\n",
        "    #img = np.array(im)\n",
        "    #plt.imshow(img, cmap=\"gray\")\n",
        "    #print(np.shape(img))\n",
        "    return img'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from matplotlib import cm\\nfrom PIL import Image, ImageOps\\ndef CutoutAbsWithPerturbation(img, v, pos_epsilon_for_attack, data_grad):  # [0, 60] => percentage: [0, 0.2]\\n    # assert 0 <= v <= 20\\n    print(\\'data_grad:\\', np.shape(data_grad))\\n    \\n    if v < 0:\\n        return img\\n    #w, h = img.size\\n    w = 28\\n    h = 28\\n    x0 = np.random.uniform(w)\\n    y0 = np.random.uniform(h)\\n\\n    #print(type(img))\\n    print(np.shape(img))\\n    #plt.imshow(img, cmap=\"gray\") # before patch is added\\n \\n    gray_img = Image.new(\\'RGB\\', (5, 5), color = \\'gray\\')\\n    grayscale_img = ImageOps.grayscale(gray_img)\\n    patch_img_no_perturbation = np.array(grayscale_img)\\n    print(\\'patch_img_no_perturbation shape:\\', np.shape(patch_img_no_perturbation))\\n    #patch_img_no_perturbation = [[0]*5 for i in range(5)] #[[]] # 5 by 5 gray image\\n    #for i in range(v):\\n      #for j in range(v):\\n        #patch_img_no_perturbation[i][j] = 0.5 # gray color is b/t 0 and 1.\\n    \\n\\n    x0 = int(max(0, x0 - v / 2.)) # x0, y0 becomes bottom left\\n    y0 = int(max(0, y0 - v / 2.))\\n    x1 = min(w, x0 + v) # x1, y1 becomes bottom right\\n    y1 = min(h, y0 + v)\\n\\n    patch_img_no_perturbation_torch = torch.from_numpy(patch_img_no_perturbation)\\n    \\n    perturbed_patch_img = fgsm_attack(patch_img_no_perturbation_torch, pos_epsilon_for_attack, data_grad)\\n    \\n    for i in range(len(img)):\\n      if (i >= x0 and i <= x1):\\n        for j in range(len(img[i])):\\n          if (j >= y0 and j <= y1):\\n            img[i][j] = .5 #perturbed_patch_img[i-x0][j-y0] #128 #(128, 128, 128) \\n\\n    #plt.imshow(img, cmap=\"gray\") # after patch is added\\n    \\n    #xy = (x0, y0, x1, y1)\\n    #color = (128, 128, 128) # gray\\n    # color = (0, 0, 0)\\n    #img = img.copy()\\n    #im = Image.fromarray(np.uint8(cm.gist_earth(img)*255))\\n    #PIL.ImageDraw.Draw(im).rectangle(xy, color)\\n    #img = np.array(im)\\n    #plt.imshow(img, cmap=\"gray\")\\n    #print(np.shape(img))\\n    return img'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FeGA4pTXLXx"
      },
      "source": [
        "pretrained_model = \"/content/patch_net.pth\" #\"/content/lenet_mnist_model.pth\"\n",
        "use_cuda=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsD60owM6H4H",
        "outputId": "5e005364-3986-4a21-ddd8-06a25c34a497"
      },
      "source": [
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X7TJcTwRKWo"
      },
      "source": [
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10) #(_, 2) means 2 classes (has patch, or no patch).\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40hI2plORGQ4",
        "outputId": "bad93a85-4fe2-4234-984b-faf0c4e0ea19"
      },
      "source": [
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "#device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "\n",
        "# FOR TESTING/VALIDATION: Set the model in evaluation mode. In this case this is for the Dropout layers\n",
        "#model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PipjNH9WWhvI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5168d4f2-509b-4853-f713-70824d2362ea"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(  # 10,000\n",
        "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            ])),\n",
        "        batch_size=1, shuffle=True)\n",
        "#print(\"len of test\", len(test_loader)) \n",
        "#train_loader = torch.utils.data.DataLoader( # 60,000\n",
        "    #datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
        "     #       transforms.ToTensor(),\n",
        "      #      ])),\n",
        "       # batch_size=1, shuffle=True)\n",
        "#print(\"len of test 2\", len(train_loader))\n",
        "\n",
        "#allmnist = [] # original 10,000 images w/o patch\n",
        "#allmnistpatch = [] # all 10,000 images w/ patches\n",
        "\n",
        "counter = 0\n",
        "\n",
        "# TESTING\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "adversarial_test_images = [] # 20,000 images. 10,000 original images, 10,000 adversarially attacked perturbed images. \n",
        "\n",
        "# Initialize the network\n",
        "model = Net()\n",
        "model.fc2 = nn.Linear(50, 2)\n",
        "\n",
        "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Load the pretrained model\n",
        "pretrained_model = \"/content/patch_net.pth\"\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "correct = 0\n",
        "for data, labels in test_loader: # 10,000 images. data is a tensor.\n",
        "  labels = torch.tensor([1])\n",
        "  counter+=1\n",
        "  img = data.squeeze().numpy() # only 1 image b/c batch_size = 1.\n",
        "  print(np.shape(img))\n",
        "  print('length of image', len(img))\n",
        "  #adversarial_test_images.append(img) # original image. part of all test images (with patch and w/o patch)\n",
        "  #print(img.size)\n",
        "  imgWithPatch, x0, y0, x1, y1 = CutoutAbs(img, 5) # gray patch coordinates\n",
        "\n",
        "  pos_eps_for_attack = .5\n",
        "  patch_data = torch.from_numpy(imgWithPatch).float() # convert back to tensor. data is the image w/ patch\n",
        "\n",
        "  patch_data, labels = patch_data.to(device), labels.to(device) # puts data on gpu\n",
        "\n",
        "  # Set requires_grad attribute of tensor. Important for Attack\n",
        "  patch_data.requires_grad = True\n",
        "  \n",
        "  #data, target = data.to(device), target.to(device)\n",
        "  #data.requires_grad = True\n",
        "  #print('data:', data)\n",
        "  print('data shape:', np.shape(patch_data))\n",
        "  patch_data = torch.unsqueeze(patch_data, 0)\n",
        "  print('data shape:', np.shape(patch_data))\n",
        "  patch_data = torch.unsqueeze(patch_data, 0) # now data is 4d which model requires\n",
        "  print('data shape:', np.shape(patch_data))\n",
        "  #print('data:', data)\n",
        "\n",
        "  output = model(patch_data)\n",
        "  \n",
        "  print('output shsape', np.shape(output))\n",
        "  init_pred = output.max(1, keepdim=True)[1]\n",
        "  print('label shape', np.shape(labels))\n",
        "  #labels = labels.unsqueeze(0)\n",
        "  #print('label shape', np.shape(labels))\n",
        "  print('output shape', np.shape(output))\n",
        "  print('output type', type(output))\n",
        "  print('output:', output)\n",
        "  print('labels:', labels)\n",
        "  #print('F.log_softmax(output):', F.log_softmax(output))\n",
        "  #print('type of F.log_softmax(output):', type(F.log_softmax(output)))\n",
        "  #print('shape of F.log_softmax(output):', np.shape(F.log_softmax(output)))\n",
        "  loss = F.nll_loss(output, labels)\n",
        "\n",
        "  # Zero all existing gradients\n",
        "  model.zero_grad()\n",
        "  \n",
        "  # Calculate gradients of model in backward pass\n",
        "  loss.backward()\n",
        "  print('output.grad', output.grad)\n",
        "  #print('data_grad', np.shape(data_grad), type(data_grad), data_grad)\n",
        "  print('data:', patch_data, 'data.grad:', patch_data.grad)\n",
        "  data_grad = patch_data.grad.data\n",
        "  print('data_grad shape before zeroing', np.shape(data_grad))\n",
        "\n",
        "  # keep patch gradients, zero everything else\n",
        "  for i in range(len(data_grad)):\n",
        "    for j in range(len(data_grad[0])):\n",
        "      if i < x0 or i > x1 or j > y0 or j < y1:\n",
        "        data_grad[i][j] = 0\n",
        "\n",
        "  pos_epsilon_for_attack = .5\n",
        "  perturbed_patch_data = fgsm_attack(patch_data, pos_epsilon_for_attack, data_grad)\n",
        "\n",
        "  # Forward pass the data through the model\n",
        "  outputs = model(perturbed_patch_data)\n",
        "\n",
        "  #apply sigmoid or softmax\n",
        "  sf = nn.Softmax(dim=1) #makes items in a row add to 1; dim = 0 makes items in a column add to 1\n",
        "  outputs = sf(outputs)\n",
        "  # Get predictions from the maximum value\n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  \n",
        "  #add labels + predictions to full set for auroc later\n",
        "  if torch.cuda.is_available():\n",
        "      outs_ones = outputs.detach().cpu().numpy()[:, 1]\n",
        "      labelsnp = labels.cpu().numpy()\n",
        "  else:\n",
        "      outs_ones = outputs.detach().numpy()[:, 1]\n",
        "      labelsnp = labels.numpy()\n",
        "\n",
        "  test_all_labels = np.append(test_all_labels, labelsnp)\n",
        "  test_all_probs_ones = np.append(test_all_probs_ones, outs_ones)\n",
        "\n",
        "  #  USE GPU FOR MODEL\n",
        "  # Total correct predictions\n",
        "  try:\n",
        "    if torch.cuda.is_available():\n",
        "        correct += (predicted.cpu() == labels.cpu()).sum()#(thresh_predicted == tlabels.cpu()).sum()\n",
        "    else:\n",
        "        correct += (predicted == labels).sum()\n",
        "  except:\n",
        "    print(\"??\")\n",
        "    correct += (predicted == labelsnp).sum()\n",
        "\n",
        "  # Total number of labels\n",
        "  try:\n",
        "      counter += len(predicted.cpu())#labels.size(0)\n",
        "  except:\n",
        "      print(\"len 0 labels\", labels)\n",
        "  accuracy = 100 * correct # counter\n",
        "  print(\"accuracy for iteration {} = {}. {} correct out of count {}\".format(i, accuracy, correct, counter))\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "length of image 28\n",
            "data shape: torch.Size([28, 28])\n",
            "data shape: torch.Size([1, 28, 28])\n",
            "data shape: torch.Size([1, 1, 28, 28])\n",
            "output shsape torch.Size([1, 2])\n",
            "label shape torch.Size([1])\n",
            "output shape torch.Size([1, 2])\n",
            "output type <class 'torch.Tensor'>\n",
            "output: tensor([[-7.7624e+00, -4.2549e-04]], grad_fn=<LogSoftmaxBackward>)\n",
            "labels: tensor([1])\n",
            "output.grad None\n",
            "data: tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.3216, 0.6784, 0.8353, 0.5176, 0.0392, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0824, 0.8745, 0.9922, 0.9882, 0.9922, 0.1961, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.6000, 0.9922, 0.7176, 0.0000, 0.6392, 0.1569, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9137, 0.9882, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,\n",
            "           1.0000, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2392,\n",
            "           0.9922, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9961, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9922, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9961, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9922, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9961, 0.9922, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,\n",
            "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.9176, 0.1176,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.9922, 0.9882, 0.4000, 0.0000, 0.0000, 0.1608, 0.6392, 0.8745,\n",
            "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.9922, 0.6745,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.7569, 0.9922, 0.7961, 0.0000, 0.2824, 0.9137, 0.9961, 0.9137,\n",
            "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 1.0000, 0.9922,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.5961, 0.9882, 0.8745, 0.0784, 0.9922, 0.9882, 0.6745, 0.1176,\n",
            "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.9922, 0.6706,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0824, 0.8745, 0.9961, 0.9922, 0.9961, 0.7529, 0.1608, 0.0000,\n",
            "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.9569, 0.1569,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.3176, 0.9922, 0.9882, 0.9922, 0.9098, 0.4000, 0.4000,\n",
            "           0.4000, 0.4000, 0.7961, 0.9529, 0.9922, 0.9882, 0.3176, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.3608, 0.9922, 1.0000, 0.9922, 1.0000, 0.9922,\n",
            "           1.0000, 0.9922, 1.0000, 0.8353, 0.7176, 0.2392, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0392, 0.6745, 0.9922, 0.6706, 0.5922, 0.9098,\n",
            "           0.9137, 0.5922, 0.5137, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.3216, 1.0000, 0.5137, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.5137, 0.9882, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<UnsqueezeBackward0>) data.grad: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-21dfe78bc9af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;31m#print('data_grad', np.shape(data_grad), type(data_grad), data_grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.grad:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mdata_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_grad shape before zeroing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUu_QZOU2M6W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcvT5gk3LTof"
      },
      "source": [
        "loss_arr = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLthk7qtHvxG"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import random\n",
        "\n",
        "class DatasetWithPatch(data.Dataset):    \n",
        "    def __init__(self, allimages, allimagespatch, alllabels, alllabelspatch, phase, transform=None):\n",
        "        \"\"\"Data loader for Model.\"\"\"\n",
        "        super(DatasetWithPatch, self).__init__()\n",
        "        \n",
        "        #train, val, test\n",
        "        self.phase = phase\n",
        "        myimgs = []\n",
        "        all_labels = []\n",
        "        startind = 0\n",
        "        endind = 0\n",
        "        print(\"hello\", len(allimages))\n",
        "        if (self.phase == \"train\"):\n",
        "          endind = 5000 #int(.6*len(allimages)) # 6000\n",
        "        elif (self.phase == \"val\"):\n",
        "          startind = int(.6*len(allimages))\n",
        "          endind = int(.8*len(allimages))\n",
        "        else: #test\n",
        "          startind = 0 #int(.8*len(allimages))\n",
        "          endind = 1000 #int(len(allimages))adversarial_test_images\n",
        "          for i in range(startind,endind):\n",
        "            #myimgs.append(allimages[i])\n",
        "            myimgs.append(adversarial_test_images[i])\n",
        "            #all_labels.append(alllabels[i])\n",
        "            all_labels.append(1) # labels are all 1s\n",
        "        print('phase:', self.phase)\n",
        "        print('myimgs length:', len(myimgs))\n",
        "        print('all_labels length:', len(all_labels))\n",
        "        print(\"startind\", startind)\n",
        "        print(\"endind\", endind)\n",
        "        \n",
        "        for i in range(startind,endind):\n",
        "          myimgs.append(allimages[i])\n",
        "          myimgs.append(allimagespatch[i])\n",
        "          all_labels.append(alllabels[i])\n",
        "          all_labels.append(alllabelspatch[i])\n",
        "        print(\"length of myimgs\", len(myimgs))\n",
        "        print(\"length of all_labels\", len(all_labels))\n",
        "        #self.imgs = myimgs\n",
        "        #self.all_labels = all_labels\n",
        "\n",
        "        #shuffle data here\n",
        "        #group images and albels together\n",
        "        #shuffle\n",
        "        #ungroup\n",
        "        temp = list(zip(myimgs, all_labels))\n",
        "        random.shuffle(temp)\n",
        "        myimgs, all_labels = zip(*temp)\n",
        "        self.imgs = myimgs\n",
        "        self.all_labels = all_labels\n",
        "\n",
        "\n",
        "    \n",
        "    #getitem is called 'batch_size' number of times in one iteration of the epoch\n",
        "    def __getitem__(self, i):\n",
        "        img_frame = self.imgs[i] #3 stacked frames (rgb) from same patient OR same image x3        \n",
        "        #create label for image\n",
        "        label = torch.LongTensor(1)\n",
        "        #print('label type[i]: ', type(self.all_labels[i]))\n",
        "        #print('label[i]: ', self.all_labels[i])\n",
        "        #print('length of label[i]: ', len(self.all_labels[i]))\n",
        "        label[0] = int(self.all_labels[i])\n",
        "\n",
        "        input1 = torch.from_numpy(img_frame).float()            \n",
        "        return {'input': input1, 'label': label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-IXlb_lxLJX"
      },
      "source": [
        "# calls init in DatasetWithPatch\n",
        "#train_set = DatasetWithPatch(allimages=allmnist, allimagespatch=allmnistpatch, \n",
        " #                alllabels=labels, alllabelspatch=labelspatch, phase=\"train\", transform=None)\n",
        "#train_set_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFjLG15MIZBc"
      },
      "source": [
        "loss_arr = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlaGuGRPxOK7"
      },
      "source": [
        "#train(model, device, train_set_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdQsbTib0EUo"
      },
      "source": [
        "#patch_path = '/content/patch_net.pth'\n",
        "#test(patch_path) # path to patch_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNuJH5vhXxeO"
      },
      "source": [
        "#np.shape(test_loader)\n",
        "#shape(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}