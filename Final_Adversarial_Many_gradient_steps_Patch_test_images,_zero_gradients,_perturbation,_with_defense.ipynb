{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Adversarial - Many gradient steps - Patch test images, zero gradients, perturbation, with defense.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkapoor11/machine-learning/blob/main/Final_Adversarial_Many_gradient_steps_Patch_test_images%2C_zero_gradients%2C_perturbation%2C_with_defense.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-uOxwBQUhhi"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "\n",
        "# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n",
        "#       see, https://github.com/pytorch/vision/issues/3497 for more information\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhuM1UiUV7-E"
      },
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "def CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n",
        "    # assert 0 <= v <= 20\n",
        "    if v < 0:\n",
        "        return img\n",
        "    #w, h = img.size\n",
        "    w = 28-v\n",
        "    h = 28-v\n",
        "    x0 = np.random.uniform(w)\n",
        "    y0 = np.random.uniform(h)\n",
        "\n",
        "    #print(type(img))\n",
        "    #print(np.shape(img))\n",
        "    #plt.imshow(img, cmap=\"gray\") # before patch is added\n",
        "\n",
        "\n",
        "  #  x0 = int(max(0, x0 - v / 2.))\n",
        "  #  y0 = int(max(0, y0 - v / 2.))\n",
        "    x1 = x0 + v #min(w, x0 + v)\n",
        "    y1 = y0 + v #min(h, y0 + v)\n",
        "    \n",
        "    for i in range(len(img)):\n",
        "      if (i >= x0 and i <= x1):\n",
        "        for j in range(len(img[i])):\n",
        "          if (j >= y0 and j <= y1):\n",
        "            img[i][j] = 0.5 #128 #(128, 128, 128)    \n",
        "\n",
        "    #plt.imshow(img, cmap=\"gray\") # after patch is added\n",
        "    return img, x0, x1, y0, y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FeGA4pTXLXx"
      },
      "source": [
        "#pretrained_model = \"/content/lenet_mnist_model.pth\"\n",
        "use_cuda=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsD60owM6H4H",
        "outputId": "5fa7458f-60a4-4450-e469-7eff73a211b1"
      },
      "source": [
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PipjNH9WWhvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71684ca-5535-4d78-c23a-7795c22b0ddd"
      },
      "source": [
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10) #(_, 2) means 2 classes (has patch, or no patch).\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# MNIST Test dataset and dataloader declaration\n",
        "#dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
        "#            transforms.ToTensor(),\n",
        "#            ]))\n",
        "\n",
        "#print(type(dataset))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            ])),\n",
        "        batch_size=1, shuffle=True) # 10,000\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            ])),\n",
        "        batch_size=1, shuffle=True) # 60,000 images\n",
        "\n",
        "allmnist = [] # original 10,000 images w/o patch\n",
        "allmnistpatch = [] # all 10,000 images w/ patches\n",
        "\n",
        "counter = 0\n",
        "x0_list = []\n",
        "x1_list = []\n",
        "y0_list = []\n",
        "y1_list = []\n",
        "\n",
        "\n",
        "for data, labels in test_loader:\n",
        "  \n",
        "  if (counter == 1):\n",
        "    break\n",
        "  counter+=1\n",
        "  img = data.squeeze().numpy() # only 1 image b/c batch_size = 1.\n",
        "  allmnist.append(img)\n",
        "  #print(img.size)\n",
        "\n",
        "  imgCopy = img.copy()\n",
        "  img_patch, x0, x1, y0, y1 = CutoutAbs(imgCopy, 5)\n",
        "  x0_list.append(x0)\n",
        "  x1_list.append(x1)\n",
        "  y0_list.append(y0)\n",
        "  y1_list.append(y1)\n",
        "\n",
        "  allmnistpatch.append(img_patch)\n",
        "\n",
        "\n",
        "  #print('printing original img')\n",
        "  #plt.imshow(img, cmap=\"gray\") \n",
        "\n",
        "  #for i in range(len(img)):\n",
        "    #print(img[i])\n",
        "  \n",
        "  if(counter % 10000 == 0):\n",
        "    plt.imshow(img_patch, cmap=\"gray\")\n",
        "    #break\n",
        "print(\"length before train added: {}, {}\".format(len(allmnist), len(allmnistpatch)))\n",
        "counter = 0\n",
        "print('done')\n",
        "print(type(allmnist))\n",
        "#allmnist = allmnist.numpy()\n",
        "print(np.shape(allmnist))\n",
        "\n",
        "\n",
        "\n",
        "labels = []\n",
        "labelspatch = []\n",
        "for i in range(len(allmnist)):\n",
        "  labels.append(0)\n",
        "  labelspatch.append(1)\n",
        "print(len(labels))\n",
        "print(len(labelspatch))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length before train added: 1, 1\n",
            "done\n",
            "<class 'list'>\n",
            "(1, 28, 28)\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUu_QZOU2M6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ad10e-71f1-4ce4-a9ee-3e7e1c4d1838"
      },
      "source": [
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "#device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "# Initialize the network\n",
        "model = Net().to(device)\n",
        "\n",
        "# Load the pretrained model\n",
        "#model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
        "\n",
        "# FOR TESTING/VALIDATION: Set the model in evaluation mode. In this case this is for the Dropout layers\n",
        "#model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcvT5gk3LTof"
      },
      "source": [
        "loss_arr = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLthk7qtHvxG"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import random\n",
        "\n",
        "class DatasetWithPatch(data.Dataset):    \n",
        "    def __init__(self, x0_list, x1_list, y0_list, y1_list, allimages, allimagespatch, alllabels, alllabelspatch, phase, transform=None):\n",
        "        \"\"\"Data loader for Model.\"\"\"\n",
        "        super(DatasetWithPatch, self).__init__()\n",
        "        \n",
        "        #train, val, test\n",
        "        self.phase = phase\n",
        "        myimgs = []\n",
        "        all_labels = []\n",
        "        startind = 0\n",
        "        endind = 0\n",
        "        \n",
        "        print(\"startind\", startind)\n",
        "        print(\"endind\", endind)\n",
        "        #for i in range(startind,endind):\n",
        "\n",
        "        # To add images with patch: \n",
        "        for i in range(len(allimagespatch)): \n",
        "          # myimgs has 10,000 (perturbed) patch images in test mode.\n",
        "          #myimgs.append(allimages[i])\n",
        "          myimgs.append(allimagespatch[i]) # this line for adding images with patch for testing. \n",
        "          #all_labels.append(alllabels[i])\n",
        "          all_labels.append(alllabelspatch[i]) # this line for adding images with patch for testing. \n",
        "        # if (self.phase == \"train\"):\n",
        "        #   # myimgs has 20,000 (unperturbed) patch and no patch images in train mode. \n",
        "        #   for i in range(len(allimagespatch)):\n",
        "        #     myimgs.append(allimages[i]) # allimages has no patches. Just regular MNIST images. \n",
        "        #     all_labels.append(alllabels[i]) # alllabels is all zeros\n",
        "        #     x0_list.append(-1)\n",
        "        #     x1_list.append(-1)\n",
        "        #     y0_list.append(-1)\n",
        "        #     y1_list.append(-1)\n",
        "        print(\"length of myimgs\", len(myimgs))\n",
        "        print(\"length of all_labels\", len(all_labels))\n",
        "        #self.imgs = myimgs\n",
        "        #self.all_labels = all_labels\n",
        "\n",
        "        #shuffle data here\n",
        "        #group images and albels together\n",
        "        #shuffle\n",
        "        #ungroup\n",
        "        temp = list(zip(myimgs, all_labels, x0_list, x1_list, y0_list, y1_list))\n",
        "        random.shuffle(temp)\n",
        "        myimgs, all_labels, x0_list, x1_list, y0_list, y1_list = zip(*temp)\n",
        "        self.imgs = myimgs\n",
        "        self.all_labels = all_labels\n",
        "\n",
        "        self.x0list = x0_list \n",
        "        self.x1list = x1_list\n",
        "        self.y0list = y0_list\n",
        "        self.y1list = y1_list\n",
        "\n",
        "    \n",
        "    #getitem is called 'batch_size' number of times in one iteration of the epoch\n",
        "    def __getitem__(self, i):\n",
        "        img_frame = self.imgs[i] #3 stacked frames (rgb) from same patient OR same image x3    \n",
        "\n",
        "        x0 = self.x0list[i]\n",
        "        x1 = self.x1list[i]\n",
        "        y0 = self.y0list[i]\n",
        "        y1 = self.y1list[i]\n",
        "\n",
        "        #create label for image\n",
        "        label = torch.LongTensor(1)\n",
        "        #print('label type[i]: ', type(self.all_labels[i]))\n",
        "        #print('label[i]: ', self.all_labels[i])\n",
        "        #print('length of label[i]: ', len(self.all_labels[i]))\n",
        "        label[0] = int(self.all_labels[i])\n",
        "\n",
        "        input1 = torch.from_numpy(img_frame).float()            \n",
        "        return {'input': input1, 'label': label, 'x0': x0, 'x1': x1, 'y0': y0, 'y1': y1 }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-IXlb_lxLJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fea741-e4df-4f39-90a5-f8df2c9302b9"
      },
      "source": [
        "# calls init in DatasetWithPatch\n",
        "train_set = DatasetWithPatch(x0_list, x1_list, y0_list, y1_list, allimages=allmnist, allimagespatch=allmnistpatch, \n",
        "                 alllabels=labels, alllabelspatch=labelspatch, phase=\"train\", transform=None)\n",
        "train_set_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startind 0\n",
            "endind 0\n",
            "length of myimgs 1\n",
            "length of all_labels 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFjLG15MIZBc"
      },
      "source": [
        "loss_arr = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70FwUiwUbuBm"
      },
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad, x0_i, x1_i, y0_i, y1_i):\n",
        "    #print('in fgsm. image shape:',np.shape(image))\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad#.sign()\n",
        "    #print('sign_data_grad b:', np.shape(sign_data_grad)) # [1,28,28]\n",
        "    sign_data_grad = torch.squeeze(sign_data_grad) \n",
        "    #print('sign_data_grad a:', np.shape(sign_data_grad))# [28,28]\n",
        "    #print('fgsm sign_data_grad:', sign_data_grad[0:20])\n",
        "    #print('in fgsm. epsilon*sign_data_grad shape:', np.shape(epsilon*sign_data_grad))\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    #print('epsilon*sign_data_grad:', epsilon*sign_data_grad)\n",
        "    '''for k in range(len(sign_data_grad)):\n",
        "      for j in range(len(sign_data_grad[0])):\n",
        "        if k < x0_i or k > x1_i or j < y0_i or j > y1_i: # outside the patch\n",
        "          continue\n",
        "          #data_grad_i[k][j] = 0\n",
        "        else:\n",
        "          #print('epsilon*sign_data_grad:', epsilon*(sign_data_grad[k][j]))'''\n",
        "\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KgzNyz-QBNu"
      },
      "source": [
        "#paste other code\n",
        "def train(model, device, train_set_loader):\n",
        "  #define criterion as crossentropy: \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #define optimizer as SGD, which is an optimization algorithm.\n",
        "\n",
        "  train_all_labels = []\n",
        "  train_all_probs_ones = []\n",
        "\n",
        "  #add final linear layer for 2 output classes\n",
        "  model.fc2 = nn.Linear(50, 2)\n",
        "\n",
        "  # Load trained weights\n",
        "  model.load_state_dict(torch.load(\"/content/patch_net.pth\", map_location='cpu')) # load patch_net weights\n",
        "\n",
        "  model.to(device) # must do this AFTER re-defining fc2 layer\n",
        "\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "  loss_arr = [] # for each epoch\n",
        "  ploss_arr = []\n",
        "\n",
        "  print(\"Printing model:\", model)\n",
        "\n",
        "  #model.train() # puts model layers in train mode. In contrast to model.eval() \n",
        "  num_epochs = 1\n",
        "  \n",
        "  model.eval() # although we are computing gradients, we are NOT updating the model by not doing the optimizer. So put the model in test mode. \n",
        "\n",
        "  for epoch in range(num_epochs): #EPOCHS. 50 OR 100 times or until validation loss starts increasing. \n",
        "    \n",
        "    running_loss = 0.0\n",
        "    prunning_loss = 0.0\n",
        "    correct = 0\n",
        "    counter = 0\n",
        "    print(\"\\nEPOCH #:\", epoch)\n",
        "    #print(\"len(train_set_loader)\", len(train_set_loader)) # 125\n",
        "    for i, data in enumerate(train_set_loader):\n",
        "      #get ur inputs and labels here\n",
        "      inputs = data['input'].to(device)\n",
        "      labels = data['label'].to(device)\n",
        "      #print('len(labels):', len(labels))\n",
        "      counter += len(inputs)\n",
        "      #if (i==0):\n",
        "        #print(\"counter should be 16:\",counter)\n",
        "      #reset gradients\n",
        "      optimizer.zero_grad() #no accumulated gradients from other batches\n",
        "      #if i % 1000 == 0:\n",
        "        #print(\"input dimensions before unsqueeze: \", inputs.size())\n",
        "      inputs = torch.unsqueeze(inputs, 1)\n",
        "      #if i % 1000 == 0:\n",
        "        #print(\"input dimensions after unsqueeze: \", inputs.size())\n",
        "      inputs.requires_grad = True\n",
        "\n",
        "      # forward + backward + optimize (to find good parameters: weights + bias)\n",
        "      outputs = model(inputs).to(device) #FEED DATA INTO MODEL\n",
        "      if (i % 100 == 0):\n",
        "        #print(\"outputs shape:\", np.shape(outputs.detach().cpu().numpy()))\n",
        "        #print(\"outputs:\", outputs.detach().cpu().numpy())\n",
        "        #print(\"labels shape before squeeze:\", np.shape(labels.cpu().numpy()))\n",
        "        labels = labels.squeeze(1) #might not need this\n",
        "        #print(\"labels shape after squeeze:\", np.shape(labels.cpu().numpy()))\n",
        "      else:\n",
        "        labels = labels.squeeze(1) #might not need this\n",
        "      \n",
        "\n",
        "      loss = criterion(outputs, labels) #calculate loss\n",
        "      loss.backward() #backpropagate loss\n",
        "      running_loss += loss.item()\n",
        "      if i % 50 == 0:    # print every 200 mini-batches\n",
        "        print(\"Running lossunperturbed:\", running_loss)\n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (i+1)))\n",
        "#            running_loss = 0.0\n",
        "      #print('here1')\n",
        "      x0_batch = data['x0'] # x0 values for 16 images\n",
        "      x1_batch = data['x1']\n",
        "      y0_batch = data['y0']\n",
        "      y1_batch = data['y1']\n",
        "      #print('here2')\n",
        "      data_grad = inputs.grad.data # stores gradients for 16 images\n",
        "      #print('len(data_grad):', len(data_grad)) # 16\n",
        "      #print('data_grad shape:', np.shape(data_grad))\n",
        "      #print('here3')\n",
        "\n",
        "      perturbed_patch_imgs = torch.zeros(0, 28, 28)#np.zeros([0, 28, 28]) # reset every batch \n",
        "      #print('here4')\n",
        "      for z in range(len(data_grad)): \n",
        "        if (labels[z] == 1): # has patch\n",
        "          img_i = inputs[z] # type is torch tensor. fgsm attack wants a torch tensor.\n",
        "          \n",
        "          img_i = torch.squeeze(img_i)\n",
        "          plt.imshow(img_i.detach().numpy(), cmap=\"gray\") \n",
        "          x0_i = x0_batch[z]\n",
        "          x1_i = x1_batch[z]\n",
        "          y0_i = y0_batch[z]\n",
        "          y1_i = y1_batch[z]\n",
        "          data_grad_i = data_grad[z]\n",
        "          #print('img_i shape:',np.shape(img_i), np.shape(img_i[0]))\n",
        "          #print(len(img_i), len(img_i[0]))\n",
        "          '''for a in range(len(img_i)):\n",
        "            for b in range(len(img_i[0])):\n",
        "              if a < x0_i or a > x1_i or b < y0_i or b > y1_i:\n",
        "                continue\n",
        "              else:\n",
        "                print('unperturbed image pixels in patch:', img_i[a][b])'''\n",
        "\n",
        "          # keep patch gradients, zero everything else\n",
        "          #print('data_grad_i shape:',np.shape(data_grad_i))\n",
        "          for k in range(len(data_grad_i)):\n",
        "            for j in range(len(data_grad_i[0])):\n",
        "              if k < x0_i or k > x1_i or j < y0_i or j > y1_i: # outside the patch\n",
        "                #print('in here')\n",
        "                data_grad_i[k][j] = 0\n",
        "                #print(data_grad_i[k][j])\n",
        "              #else:\n",
        "                #print('pixel gradient [', k,'][', j, '] in patch:', data_grad_i[k][j])\n",
        "\n",
        "          #print('done zeroing patch gradients')\n",
        "          neg_eps_for_attack = -0.01\n",
        "          pos_epsilon_for_defense = .1\n",
        "          perturbed_patch_img_defense = fgsm_attack(img_i, pos_epsilon_for_defense, data_grad_i, x0_i, x1_i, y0_i, y1_i)\n",
        "          num_gradient_steps = 1000\n",
        "          perturbed_patch_img = img_i\n",
        "          print(np.shape(perturbed_patch_img))\n",
        "          print(type(perturbed_patch_img))\n",
        "          probabilities = []\n",
        "          for i in range(num_gradient_steps):\n",
        "            #print('here1', np.shape(perturbed_patch_img))\n",
        "            perturbed_patch_img = fgsm_attack(perturbed_patch_img, neg_eps_for_attack, data_grad_i, x0_i, x1_i, y0_i, y1_i)\n",
        "            #print('here2', np.shape(perturbed_patch_img))\n",
        "            perturbed_patch_img = torch.squeeze(perturbed_patch_img) # squeeze to get to 28 by 28\n",
        "            #print('here3', np.shape(perturbed_patch_img))\n",
        "            perturbed_patch_img = torch.unsqueeze(perturbed_patch_img, 0)\n",
        "            #print('here4', np.shape(perturbed_patch_img))\n",
        "            perturbed_patch_imgs = torch.cat((perturbed_patch_imgs, perturbed_patch_img))\n",
        "            #print('here5', np.shape(perturbed_patch_img))\n",
        "            # squeeze perturbed_patch_img again so\n",
        "            perturbed_patch_img = torch.squeeze(perturbed_patch_img)\n",
        "            perturbed_patch_imgs = torch.unsqueeze(perturbed_patch_imgs, 1)\n",
        "            #print('here6', np.shape(perturbed_patch_img))\n",
        "            poutputs = model(perturbed_patch_imgs)\n",
        "            perturbed_patch_imgs = torch.zeros(0, 28, 28)\n",
        "            sf = nn.Softmax(dim=1) #makes items in a row add to 1; dim = 0 makes items in a column add to 1\n",
        "            poutputs = sf(poutputs)\n",
        "            #print('image', i, 'softmax', poutputs.detach().cpu().numpy()) #poutputs[0], poutputs.detach().cpu().numpy()[:, 1])\n",
        "            probabilities.append(poutputs.detach().cpu().numpy()[:, 1][0])\n",
        "            #print('here7', np.shape(perturbed_patch_img))\n",
        "          print('outside')\n",
        "          perturbed_patch_img = torch.squeeze(perturbed_patch_img) # squeeze to get to 28 by 28\n",
        "          #print('here1')\n",
        "          #if (i % 50 == 0):\n",
        "          #print(x0_i, x1_i, y0_i, y1_i)\n",
        "          '''for a in range(len(perturbed_patch_img)):\n",
        "            #print('here1')\n",
        "            for b in range(len(perturbed_patch_img[0])):\n",
        "              #print('here2')\n",
        "              if a < x0_i or a > x1_i or b < y0_i or b > y1_i: # outside patch\n",
        "                #print('here3')\n",
        "                #print('in here')\n",
        "                print('image pixels outside patch:[',a,'][',b,']',perturbed_patch_img[a][b])\n",
        "                #continue\n",
        "             # else:\n",
        "                #print('perturbed_patch_img shape:',np.shape(perturbed_patch_img), np.shape(perturbed_patch_img[0]))\n",
        "                #print('image pixels in patch:[',a,'][',b,']',perturbed_patch_img[a][b])\n",
        "          '''\n",
        "          perturbed_patch_img = torch.unsqueeze(perturbed_patch_img, 0) # back to [1,28,28]\n",
        "          perturbed_patch_imgs = torch.cat((perturbed_patch_imgs, perturbed_patch_img))#.append(perturbed_patch_img)\n",
        "          \n",
        "          #if(i%100==0):\n",
        "           # print(np.shape(perturbed_patch_imgs))\n",
        "      \n",
        "      '''for z in range(len(data_grad)):\n",
        "        #if (labels[z] == 1): # has patch\n",
        "          img_i = inputs[z] # type is torch tensor. fgsm attack wants a torch tensor.\n",
        "          perturbed_patch_imgs = torch.cat((perturbed_patch_imgs, img_i))#.append(perturbed_patch_img)\n",
        "      '''\n",
        "      #print('here5')\n",
        "      perturbed_patch_imgs = torch.unsqueeze(perturbed_patch_imgs, 1) # to [batch_size,1,28,28]\n",
        "      #print('perturbed_patch_imgs:', np.shape(perturbed_patch_imgs), len(perturbed_patch_imgs))\n",
        "      #perturbed_patch_imgs = torch.unsqueeze(perturbed_patch_imgs, 1)\n",
        "      #print('here6')\n",
        "      poutputs = model(perturbed_patch_imgs)#inputs) # just inputs instead of perturbed_patch_imgs will be just unperturbed, patch and no patch images. #perturbed_patch_imgs) \n",
        "      #print('poutputs:', np.shape(poutputs), len(poutputs))\n",
        "      #print('here7')\n",
        "      #print('len(perturbed_patch_imgs):', len(perturbed_patch_imgs))\n",
        "      #print(len(perturbed_patch_imgs))\n",
        "\n",
        "      plabels = torch.ones((len(perturbed_patch_imgs))).long() #labels[:len(perturbed_patch_imgs)]\n",
        "      #print('plabels:', np.shape(plabels), len(plabels))\n",
        "      #print(len(perturbed_patch_imgs))\n",
        "      #print('here8')\n",
        "      #print(len(poutputs), len(plabels))\n",
        "      ploss = criterion(poutputs, plabels) #calculate loss\n",
        "      #print('here9')\n",
        "      prunning_loss += ploss.item()\n",
        "      #print('here10')\n",
        "      if i % 50 == 0:    # print every 200 mini-batches\n",
        "        print(\"Running loss Perturbed:\", prunning_loss)\n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, prunning_loss / (i+1)))\n",
        "        print('poutputs[0]', poutputs[0])    \n",
        "\n",
        "      #apply sigmoid or softmax\n",
        "      sf = nn.Softmax(dim=1) #makes items in a row add to 1; dim = 0 makes items in a column add to 1\n",
        "      poutputs = sf(poutputs)\n",
        "      print('softmax', poutputs[0])\n",
        "\n",
        "      #if i % 500 == 0:\n",
        "        #print(\"len poutputs\", np.shape(poutputs), \"poutputs[0]:\", poutputs[0]) # length of outputs is batch_size (16) by # of classes (2).\n",
        "      \n",
        "      # Get predictions from the maximum value\n",
        "      _, predicted = torch.max(poutputs, 1)\n",
        "\n",
        "      # #add labels + predictions to full set for auroc later\n",
        "      if torch.cuda.is_available():\n",
        "          outs_ones = poutputs.detach().cpu().numpy()[:, 1]\n",
        "          plabelsnp = plabels.cpu().numpy()\n",
        "          correct += (predicted.cpu() == plabels.cpu()).sum()\n",
        "      else:\n",
        "          outs_ones = poutputs.detach().numpy()[:, 1]\n",
        "          plabelsnp = plabels.numpy()\n",
        "          correct += (predicted == plabels).sum()\n",
        "\n",
        "      # counter += len(predicted.cpu())\n",
        "\n",
        "      accuracy = 100 * correct // counter\n",
        "      if (i % 100 == 0):\n",
        "        print(\"accuracy for iteration {} = {} %. {} correct out of count {}\".format(i, accuracy, correct, counter))\n",
        "      \n",
        "    PATH = '/content/patch_net_adv.pth'\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    ploss_arr.append( prunning_loss / len(train_set_loader))\n",
        "  # plot loss\n",
        "  plt.figure(figsize=(5,5))\n",
        "  # plt.plot(ploss_arr)\n",
        "  plt.plot(probabilities)\n",
        "  plt.plot()\n",
        "  # plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "  # plt.xticks(np.arange(0, .5, step=0.1))\n",
        "  plt.title(\"Probability of patch for {} gradient steps and pos epsilon = {} and neg eps = {}\".format(num_gradient_steps, pos_epsilon_for_defense, neg_eps_for_attack))\n",
        "  plt.xlabel(\"Gradient step\")\n",
        "  plt.ylabel(\"Softmax probability of 1 (has patch)\")\n",
        "  # plt.show()\n",
        "  return perturbed_patch_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlaGuGRPxOK7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "cf2719a2-f431-4ef4-de24-e175e72fb3ee"
      },
      "source": [
        "print()\n",
        "perturbed_patch_imgs = train(model, device, train_set_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Printing model: Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "EPOCH #: 0\n",
            "Running lossunperturbed: 0.5461991429328918\n",
            "[1,     1] loss: 0.546\n",
            "torch.Size([28, 28])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "outside\n",
            "softmax tensor([0.4209, 0.5791], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAFNCAYAAABG2sb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debwcVZn/8c+XBIisYYn8IAkEJKCgYbsC7shmBAmoqOg4gojIKOIgLqAzI4Ki4L6gDCICyioiBkZEdkQFcsOeIBACmLDGhCUsAsHn98c5l1Q63XfLPV333nzfr1e/uuvU0k+t/fSpU1WKCMzMzMxKWKHuAMzMzGz4cqJhZmZmxTjRMDMzs2KcaJiZmVkxTjTMzMysGCcaZmZmVkxbEw1JIWnTfo57v6RdW/R7i6S7mg0r6UuSTulfxH2O8d2S5kh6WtI27fjOFnGcJulrfRh+UMQ91Eg6QNJ1le6nJW1SZ0zDgaSdJM2tO46BVj0WSZqQj4cj645rqFmW3xGrR4+JRv7Rfi4fRB/NP2KrtSO43oqIP0XE5i36HRcRB0Fbdu5vA4dGxGoRcfNATri7RGsADGjckt4v6S+SnpV0dZP+W0uanvtPl7R1pZ8kHS9pfn4dL0m9GbduefnNXtbp9DVRHKhxrazqsWgw6su+JelQSZ2Snpd0WhvDtH6QtLKkUyU9JekRSZ/tYfjD83BP5fFWrvQ7VtLtkhZJOro339/bGo29ImI1YFugA/ivJoE5M4eNgBl1B9EP/Y5b0ogmxQuA7wPfbDL8SsDvgF8BawGnA7/L5QAHA/sAWwGTgL2AT/Ry3H7z9mvLs37sWw8BXwNObU+EtoyOBiaSjvVvB74gaXKzASW9AzgS2CUPvwnw1cogs4AvAP/X62+PiG5fwP3ArpXubwEX588BfAq4B7gvl308B7IAmApsUBk3gMOA2cA/8rRWyP1eBVwJzM/9zgRGN8RxFDATeBz4BTAq99sJmNss5ryAf5U//z3H8HR+vS3H+brKuK8EngXGNFkWK5CSrAeAx4AzgDWBlfP0AngGuLfFsuzX/AO/BP4FPJe/5wu5/M3AX4AngDnAAbn8NOBE0oawELgBeFWTeJrGDbwGuDpPdwYwpTLOacBPgd/ncXZtNq952IOAqxvKdgceBFQp+zswOX/+C3Bwpd/HgOt7M26T798YuDYvg8vzMunaFibk+f5Ynsa1ufzXwCPAk3ncLSvTW4e0TT8F3AgcC1zXsH43rSzbb+dpPwqcBLyiur0CR+Tt6GHgo7nfwcCLwAt53VzUZL4EfC+P+xRwO/DaVuMCGwC/AeYB9wGHVaZ1NHA+cG5eTjcBW1X6fzEv84XAXcAuLZb1nsDNOZ45wNGVfl3Lev+8PP4BfLnS/xV5u3qctH9/nsr+3Mf9qOk+mvuNIv2Qzidt29OA9Vp8x4AvM5Y8FnUtk5GV75tKOh7NAj7e8H3n5XlZSNonO3o6dvflRR/3rcowXwNO62GY3hzbPwfcRtrvziUf23P/z5P2kYeAA6nsZ02+62rSfvnnvKz+CKxb6b8ji4+ZtwI79eZ40eK73gXckqf1F2BSwzy1+r1aF7g4j7cA+BN5+y31ystu90r3scA5LYY9Cziu0r0L8EiT4X5FZT/v9vt7EeD9LP7RHp838mMrO/xlwNqkg8XOeUPalnSg/RH5AF4Z/qo8/IbA3cBBud+mwG55vDF5hX+/IY47cgxr5w3pa7nfTvQu0ZhAZefOZT8Bjq90f4YmB/fc70DSQWATYDXgAuCXDfPXdAcYoPmvJnwbkXaIDwIrkn4Et879TiPt1NsDI0k7dtONqjHuPK1ZwJeAlfI6XQhsXpn2k8CbSAf1Ud1Mt1micThwSUPZxcAR+fOTwA6Vfh3Awt6M2+T7/0r6sV+JlJQ91WRbOANYlcVJwIHA6nk9fB+4pTK9c0gH/FVJP+wP0jrR+B7ph2PtPL2LgG9UttdFwDF5ee9BSm7Xqizjr3WzXN8BTAdGk5KO1wDrNxs3r6PpwP/k5bAJ6Qf6HZX940Vg3xzL50g/rCsCm5OShg0qy2yphLUyT6/L3zeJlFzt07Csf0Y6TmwFPA+8Jvf/JulguzZp/76DnhONVvtRy32UVDN2EbAKMALYDlijyfSLLDO6TzSuJR2LRgFbkxKcnSvj/ZO0nYwAvkFOvlssn9tIP2LNXj9pMU6f9q3KML1JNHpzbLuRlGytDdwJHJL7Tc7b0mtJ+91Z9Jxo3Atslre1q4Fv5n5jScfFPfI63i13j+npeNHke7YhJbI75HWyf56PlSvz1Or36hukPx4r5tdbqCR4y7oum0xjrbzM1quU7Qvc3mL4W4EPVLrXzeOv0zDcgCcaT+cZe4C0M3QdlIO8M+TunwMnVLpXI+2QEyrDT670/yRwRYvv3Qe4uSGOQyrde7D4H/hO9D/R2IGUuSt3dwLvbxHTFcAnK92b5/kbWZm/nhKNZZn/aqJxFPDbFuOeBpzSsKz+1kNcXT+QbyH9o1+h0v/srg0qT/uMXm7gzRKN/6Yh6SElQl3Tfwl4daXfxByfehq3oXxD0o/5Kg07RuO2sEk38Y/Ow6xJOpi82BDbcTRJNHKsz1D5UQbewOJav51ItVPV7fAxYMfKMu4u0diZ9OO6Iw3/hBrH7dq+G4Y5CvhFZf+4vtJvBdK/x7fkeXkM2BVYsTfrvDKd7wPfa1jW4yr9bwT2y59ns+R+cTA9JxpN9yO62UdJScgS/zxbTL/IMqPFsYj0Y/QSsHpl2G+Qf8DzeJdX+m0BPNeX9dGL9dXrfathmB4TjSbjNDu2fbjSfQJwUv58KjlRyN2b0XOi8V8N28Yf8ucvUvljmMsuJSUJ3R4vmnzPT8l/uCtldwFvq8xTq9+rY0inqVr+Vgzwuh2fl1m1lmg34P4Ww9/bsH+tmMef0DBcrxON3rbR2CciRkfERhHxyYh4rtJvTuXzBqRkBICIeJqUMY5tMfwDeRwkrSfpHEkPSnoqz8S6DXE0HXdZRMQNpH+TO0l6NelAMbXF4EvMX/48ElivD1+5LPNfNZ60QbTySOXzs6Skrzc2AOZExL8a4my1DvvqaWCNhrI1SLUmzfqvATwdacvuadyqDYAFEfFspaxZ3C+XSRoh6ZuS7s3r4P7ca13SP7GRLL3+mhlD+tc8XdITkp4A/pDLu8yPiEWV7l6vo4i4EvgxqWr3MUknS2pcLl02AjboiiPH8iWW3GZfnqe83ueS/pHPAv6T9EP3WN4+m+5zknaQdJWkeZKeBA5h6e231Ta5Ab1brlWtjgXd7aO/JP2wnCPpIUknSFqxybTbsswqurbV6nbcuM81LrtRA9yuqC/7Vp/08tg2kNtGq2ltBLyvYb2+GVif3h8vumwEHNEwrfEs+ZvUahv9FqnW7Y+SZks6shfz1GuSTsoXbzwt6UukdQtLH1dbrdtmx2C6Gb5HA3F5a1Q+P0RaAQBIWpVUpf9gZZjxlc8b5nEg/TsMUnuJNYAPk/4Z0otx+xNr1en5+/4dOD8i/tliuCXmj8VZ8KN9iKG/898Y+xzSuc+B9hAwXlJ129iQJddhq+XYGzOASdUrSUhV7TMq/beq9NuqoV9341Y9DKwtaZVK2fgmw1Xn5UPA3qR/o2uS/nVCWg/zSOu6cf018w9SjcWWOUEfHRFrRmpQ3Rs9Lt+I+GFEbEf6d7sZ6Tx2s3HnkGpSRldeq0fEHpVhXp6nvN7HkbfLiDgrIt5M2u4DOL5FSGeREvTxEbEmqWq4cf9t5WF6t1yrWu1HLffRiHgxIr4aEVsAbySdY/9Ik2m3a5l1eYi0ra7eEPeDLYbvlqQZlR+axtdJLUbry77VV705trfSn22jlTmkGo3qel01Ir5J748X1Wl9vWFaq0TE2S3Gf3kbjYiFEXFERGwCTAE+K2mXZl/Sn3UZEYdEugJutUhXOj2e56/VcbVRs2PwoxExv/Xi6N5A30fjbOCj+TKplUkb2A0RcX9lmM9LWkvSeFJ7iHNz+eqkTOpJSWNZfOCs+pSkcZLWBr5cGbe35pEaVTbe6+BXwLtJO8AZ3Yx/NnC4pI2VLvE9Dji34Z9pT/o7/482xH0msKvSpaQjJa3T3eVofdBVw/MFSStK2ol05cc5vZ1ArhkYRfonuYKkUZV/jleTqokPU7rk6tBcfmV+P4O0443N/wSPIJ0O6M24L4uIB0inwY6WtJKkN+T56M7qpLYD80k1EsdVpvcS6Xz/0ZJWkbQFqcp1Kfkf7s+A70l6ZV4mY3Nr7t5oXNdLkPT6XIOwIukUzT9J23WzcW8EFkr6oqRX5HXzWkmvrwyznaT35H/I/5mXwfWSNpe0c96X/0lKnqo1XVWrk/4R/lPS9qSkrbfOA47K+8U44NO9GKfVftRyH5X0dkmvU7pS6inSKZVm89OuZQZARMwhndL5Rt5XJpEaKf+qF8uh2fS2rPzQNL4OaTHa1fRy34J0lVbex0cAI3LcrWpYenNsb+U84ABJW+Qk4Ct9GLfRr4C9JL2j6xildM+Wcf04XvwMOCTvh5K0qqQ9G5LFpr9Xkt4ladOc1D1JWu5Nt5F+rstmzgD+K+8zryZdtHFaN8N+LC/z0aTG1S8Pm38XRpHyh5F5OTa7+nCJGenp/M79tLiygCbnykhVpveSWtNezJLnZYPFrcXnA98BRuR+W5IaYD1Nasl7BEu3u+hqxfsEqRZildxvpybDLtVGIxafH5uXp7FjpfzyPF7TRjl5mBVIDcTm5Gn8ityAr9XyaLK8+jv/e5PakjwBfC6XvYWUGHS19N8/l5/Gkufpl1g+Pa3HHMs1pJ1gJvDuSr8lpt1iegfkaVZfp1X6b5Pn9TlSi/1tKv1EOke7IL9OYMmW8C3HbRLHq0iNDBeSzt2fDPw895vA0u11ViOdO11Iqur8SHXZkE59XEzvrjoZRfqRm52Hv5N85UKz9cGS2+xEFrdmv7DJfO1CaiT2NItb8a/WalxSle3ZpCrlx4HrWXL/qF5BcTOwbe43Kc/nQhbvzxu0WNb75mW2MA/3Y7pvG3U1ixtwrkI6uD1B3686adyPWu6jpIbTd5GSs0eBH1ZjaviOAV9mdN8YdFwedgHp+Fk9v//yeK2W50C86H6//BKVxqI5psZ9vOn5enp3bN+1YdrV+T0yr4feXnVyUMOxqLqP7kA6ti3I28f/ARv2dLxo8V2TSVcuPUGqMfg1uZ0N3f9eHZ77P0M65fbfA7keW8S6Mqm9y1Okbf+zlX4b5nWzYaXss3m4p0hXzKxc6Xdak3V/QHff39UAcrkn6VTgoYhY6h4hA/gdAUyMdB7X2kzSuaRGscvyr2hYUbrhzqYR8eG6Y+mtuvejobjMrO+W5Xgh6X5SwnP5gAc2BPlZJ6Q7hgLvIV01Y8NEPsXwKkkrKN2cZm/gwrrjMrPBx8eLcpb7uyFKOpZUlfWNiLiv7nhsQP0/UruKdUhVlP8RA3xreDMbNny8KMSnTszMzKwYnzoxMzOzYpxomJmZWTHLfRuNwSo3RvoB6Tr1UyLdVKZxmPez+DKzWyPiQ7n8D6TbU18XEe/qzfetu+66MWHChIEJ3swsmz59+j8iYkzPQ9pw5URjEMo3PzmRdD/6ucA0SVMjYmZlmImk67TfFBGPd90YKvsW6d4En+jtd06YMIHOzs4Bid/MrIuk3tw23IYxnzoZnLYHZkXE7Ih4gXRXzr0bhvk4cGKk28sSEY919YiIKxiAZxSYmZktKycag9NYlnwgz1yWfMASpOdbbCbpz5Kuz6dazMzMBhWfOhm6RpJuN70T6fbF10p6XUQ80dsJSDqY9EhuNtxwWZ5VZGZm1pxrNAanB1nyyX/jWPpJjnOBqZGeSHkfcDcp8ei1iDg5IjoiomPMGLfVMjOzgedEY3CaBkzMT6BcCdiP9AjuqgtJtRlIWpd0KmV2O4M0MzPriRONQSjSY+cPBS4lPfXzvIiYIekYSVPyYJcC8yXNBK4CPh8R8wEk/Yn0JMFdJM3tw+PJzczMBpRvQW4AdHR0hC9vNbOBJml6RHTUHYfVxzUaZmZmVowTDTMzMyvGiYaZmZkV40TDzMzMinGiYWZmZsU40TAzM7NinGiYmZlZMU40zMzMrBgnGmZmZlaMEw0zMzMrxomGmZmZFeNEw8zMzIpxomFmZmbFONEwMzOzYpxomJmZWTFONMzMzKwYJxpmZmZWjBMNMzMzK8aJhpmZmRXjRMPMzMyKcaJhZmZmxTjRMDMzs2KcaJiZmVkxTjTMzMysGCcaZmZmVowTDTMzMyvGiYaZmZkV40TDzMzMinGiYWZmZsU40TAzM7NinGiYmZlZMU40zMzMrBgnGmZmZlaMEw0zMzMrxonGICVpsqS7JM2SdGSLYd4vaaakGZLOqpTvL+me/Nq/fVGbmZktaWTdAdjSJI0ATgR2A+YC0yRNjYiZlWEmAkcBb4qIxyW9MpevDXwF6AACmJ7Hfbzd82FmZuYajcFpe2BWRMyOiBeAc4C9G4b5OHBiVwIREY/l8ncAl0XEgtzvMmBym+I2MzNbghONwWksMKfSPTeXVW0GbCbpz5KulzS5D+OamZm1hU+dDF0jgYnATsA44FpJr+vLBCQdDBwMsOGGGw50fGZmZq7RKEnSOEmfk/Q7SdMkXSvpJ5L2lNTdsn8QGF/pHpfLquYCUyPixYi4D7iblHj0ZlwAIuLkiOiIiI4xY8b0dfbMzMx65ESjEEm/AE4FXgCOBz4IfBK4nNRm4jpJb20x+jRgoqSNJa0E7AdMbRjmQlJtBpLWJZ1KmQ1cCuwuaS1JawG75zIzM7O286mTcr4TEXc0Kb8DuCAnEE3PV0TEIkmHkhKEEcCpETFD0jFAZ0RMZXFCMRN4Cfh8RMwHkHQsKVkBOCYiFgzonJmZmfWSIqLuGGwQ6OjoiM7OzrrDMLNhRtL0iOioOw6rj2s0CpP0JuBoYCPS8hYQEbFJnXGZmZm1gxON8n4OHA5MJ53iMDMzW2440SjvyYi4pO4gzMzM6uBEoxBJ2+aPV0n6FnAB8HxX/4i4qZbAzMzM2siJRjnfaeiuNoYKYOc2xmJmZlYLJxqFRMTb647BzMysbr5hV2GSjpM0utK9lqSv1RmTmZlZuzjRKO+dEfFEV0d+ouoeNcZjZmbWNk40yhshaeWuDkmvAFbuZngzM7Nhw200yjsTuCI/+wTgo8AZNcZjZmbWNk40CouI4yXdCuyai46NCD/kzMzMlgtONAqTdHxEfBH4Q5MyMzOzYc1tNMrbrUnZO9sehZmZWQ1co1GIpP8APglsIum2Sq/VgT/XE5WZmVl7OdEo5yzgEuAbwJGV8oURsaCekMzMzNrLiUYhEfEk8CTwQQBJrwRGAatJWi0i/l5nfGZmZu3gNhqFSdpL0j3AfcA1wP2kmg4zM7Nhz4lGeV8DdgTujoiNgV2A6+sNyczMrD2caJT3YkTMB1aQtEJEXMWST3I1MzMbttxGo7wnJK0G/Ak4U9JjwDM1x2RmZtYWrtEob2/gOeA/STftuhfYq9aIzMzM2sQ1GoVFxDOS/h+wPbAAuDSfSjEzMxv2XKNRmKSDgBuB9wD7AtdLOrDeqMzMzNrDNRrlfR7YpqsWQ9I6wF+AU2uNyszMrA1co1HefGBhpXthLjMzMxv2XKNR3izgBkm/A4LUOPQ2SZ8FiIjv1hmcmZlZSU40yrs3v7r8Lr+vXkMsZmZmbeVEo7CI+GrdMZiZmdXFbTTMzMysGCcaZmZmVowTDTMzMyvGiUZhkk6QtIakFSVdIWmepA/XHZeZmVk7ONEob/eIeAp4F3A/sCnpJl5mZmbDnhON8rqu7NkT+HVEPFlnMGZmZu3kRKO8iyX9DdgOuELSGOCfPY0kabKkuyTNknRkk/4H5NMwt+TXQZV+x0u6I78+MKBzY2Zm1ge+j0ZhEXGkpBOAJyPiJUnPkO4O2pKkEcCJwG7AXGCapKkRMbNh0HMj4tCGcfcEtgW2BlYGrpZ0ST59Y2Zm1lZONNpjA2BXSaMqZWd0M/z2wKyImA0g6RxSctKYaDSzBXBtRCwCFkm6DZgMnNevyM3MzJaBT50UJukrwI/y6+3ACcCUHkYbC8ypdM/NZY3eK+k2SedLGp/LbgUmS1pF0rr5O8c3GdfMzKw4Jxrl7QvsAjwSER8FtgLWHIDpXgRMiIhJwGXA6QAR8Ufg96RH0Z8N/BV4qdkEJB0sqVNS57x58wYgJDMzsyU50SjvuYj4F+k0xhrAY/Rcw/BgwzDjctnLImJ+RDyfO08hNTbt6vf1iNg6InYDBNzd7Esi4uSI6IiIjjFjxvRppszMzHrDiUZ5nZJGAz8DpgM3kWoZujMNmChpY0krAfsBU6sDSFq/0jkFuDOXj5C0Tv48CZgE/HEgZsTMzKyv3Bi0sIj4ZP54kqQ/AGtExG09jLNI0qHApcAI4NSImCHpGKAzIqYCh0maAiwCFgAH5NFXBP4kCeAp4MO5YaiZmVnbKSLqjmHYkzQW2IhKYhcR19YX0dI6Ojqis7Oz7jDMbJiRND0iOuqOw+rjGo3CJB0PfIB0aWpXo8wABlWiYWZmVoITjfL2ATavNNw0MzNbbrgxaHmzSe0mzMzMljuu0ShE0o9Ip0ieBW6RdAXwcq1GRBxWV2xmZmbt4kSjnK6WldNpuDTVzMxseeFEo5CIOL3uGMzMzOrmNhqFSLpI0l6SlmqfIWkTScdIOrCO2MzMzNrFNRrlfBz4LPB9SQuAecAoYGNgFvDjiPhdjfGZmZkV50SjkIh4BPgC8AVJE4D1geeAuyPi2RpDMzMzaxsnGm0QEfcD99cchpmZWdu5jYaZmZkV40TDzMzMinGiYWZmZsU40aiBpEvqjsHMzKwd3Bi0EEnbtuoFbN3OWMzMzOriRKOcacA1pMSi0eg2x2JmZlYLJxrl3Al8IiLuaewhaU4N8ZiZmbWd22iUczStl++n2xiHmZlZbVyjUUhEnN9NvwvbGYuZmVldXKNhZmZmxTjRMDMzs2KcaBQi6X35feO6YzEzM6uLE41yjsrvv6k1CjMzsxq5MWg58yX9EdhY0tTGnhExpYaYzMzM2sqJRjl7AtsCvwS+U3MsZmZmtXCiUUhEvABcL+mNETFP0mq5/OmaQzMzM2sbt9Eobz1JNwMzgJmSpkt6bd1BmZmZtYMTjfJOBj4bERtFxIbAEbnMzMxs2HOiUd6qEXFVV0dEXA2sWl84ZmZm7eM2GuXNlvTfpEahAB8GZtcYj5mZWdu4RqO8A4ExwAWke2qsm8vMzMyGPddoFBYRjwOH1R2HmZlZHVyjYWZmZsU40RikJE2WdJekWZKObNL/AEnzJN2SXwdV+p0gaYakOyX9UJLaG72ZmVniUyeFSVonIub3cZwRwInAbsBcYJqkqRExs2HQcyPi0IZx3wi8CZiUi64D3gZc3Y/wzczMlolrNMq7XtKvJe3Rh5qF7YFZETE732H0HGDvXo4bwChgJWBlYEXg0b4GbWZmNhCcaJS3GekGXf8O3CPpOEmb9TDOWGBOpXtuLmv0Xkm3STpf0niAiPgrcBXwcH5dGhF3LutMmJmZ9YcTjcIiuSwiPgh8HNgfuFHSNZLesAyTvgiYEBGTgMuA0wEkbQq8BhhHSk52lvSWZhOQdLCkTkmd8+bNW4ZQzMzMmnOiUZikdSR9RlIn8Dng06R7aRwBnNVitAeB8ZXucbnsZRExPyKez52nANvlz+8Gro+Ip/MD3C4BmiY0EXFyRHRERMeYMWP6MXdmZmbdc6JR3l+BNYB9ImLPiLggIhZFRCdwUotxpgETJW0saSVgP2BqdQBJ61c6pwBdp0f+DrxN0khJK5IagvrUiZmZ1cJXnZT3XxFxXrVA0vsi4tcRcXyzESJikaRDgUuBEcCpETFD0jFAZ0RMBQ6TNAVYBCwADsijnw/sDNxOahj6h4i4aKBn6qsXzWDmQ08N9GTNbBDbYoM1+MpeW9Ydhg0xTjTKOxI4r6HsKODX3Y0UEb8Hft9Q9j+Vz0fl6TSO9xLwif4Ga2ZmNpCcaBQi6Z3AHsBYST+s9FqDVAsxpPlfjZmZ9YYTjXIeAjpJ7SemV8oXAofXEpGZmVmbOdEoJCJuBW6VdGZEDPkaDDMzs/5wolGIpPMi4v3AzZKisX++/4WZmdmw5kSjnM/k93fVGoWZmVmNnGgUEhEP5/cH6o7FzMysLk40CpG0kHQfi6V6ke5MvkabQzIzM2s7JxqFRMTqdcdgZmZWNycahUhaIyKekrR2s/4RsaDdMZmZmbWbE41yziI1BJ1OOoWiSr8ANqkjKDMzs3ZyolFIRLwrv29cdyxmZmZ1caLRBpLeA7yZVJPxp4i4sOaQzMzM2sKPiS9M0k+AQ0hPU70DOETSifVGZWZm1h6u0ShvZ+A1EREAkk4HZtQbkpmZWXu4RqO8WcCGle7xuczMzGzYc41GIZIuIrXJWB24U9KNuXsH4MY6YzMzM2sXJxrlfLvuAMzMzOrmRKOQiLim7hjMzMzq5jYahUnaUdI0SU9LekHSS5KeqjsuMzOzdnCiUd6PgQ8C9wCvAA4CfHmrmZktF5xotEFEzAJGRMRLEfELYHLdMZmZmbWD22iU96yklYBbJJ0APIwTPDMzW074B6+8fyct50OBZ0j30XhvrRGZmZm1iWs0CouIB3KNxgTgAuCuiHih3qjMzMzaw4lGYZL2BE4C7iU9Kn5jSZ+IiEvqjczMzKw8JxrlfQd4e24QiqRXAf8HONEwM7Nhz200ylvYlWRks4GFdQVjZmbWTq7RKETSe/LHTkm/B84jPevkfcC02gIzMzNrIyca5exV+fwo8Lb8eR4wqv3hmJmZtZ8TjUIi4qN1x2BmZlY3t9EoTNI4Sb+V9Fh+/UbSuLrjMjMzawcnGuX9ApgKbJBfF+UyMzOzYc+JRnljIuIXEbEov04DxtQdlJmZWTs40ShvvqQPSxqRXx8G5tcdlJmZWTs40SjvQOD9wCOkB6rtC/TYUFTSZEl3SZol6cgm/Q+QNE/SLfl1UC5/e6XsFkn/lLTPAM+TmZlZr/iqk4IkjQCOi4gp/RjvRGA3YC4wTdLUiJjZMOi5EXFotSAirgK2ztNZG5gF/LGfs2BmZrZMXKNRUES8BGyUH6rWF9sDsyvvNVMAABCeSURBVCJidn4A2znA3v0IYV/gkoh4th/jmpmZLTPXaJQ3G/izpKmkx8QDEBHf7WacscCcSvdcYIcmw71X0luBu4HDI2JOQ//9gO6+x8zMrCjXaJR3L3AxaVmvXnktq4uACRExCbgMOL3aU9L6wOuAS1tNQNLBkjoldc6bN28AQjIzM1uSazQKi4ivAkhaI3VGbx6o9iAwvtI9LpdVp1u9cuUU4ISGabwf+G1EvNhNbCcDJwN0dHREL+IyMzPrE9doFCapQ9LtwG3A7ZJulbRdD6NNAyZK2ji379iPdNOv6nTXr3ROAe5smMYHgbOXLXozM7Nl4xqN8k4FPhkRfwKQ9GbSnUEntRohIhZJOpR02mMEcGpEzJB0DNAZEVOBwyRNARYBC4ADusaXNIFUI3JNiRkyMzPrLUW4xrwkSTdHxDYNZTdFxLZ1xdRMR0dHdHZ21h2GmQ0zkqZHREfdcVh9XKNR3jWS/pd0GiOADwBXS9oWICJuqjM4MzOzkpxolLdVfv9KQ/k2pMRj5/aGY2Zm1j5ONAqLiLfXHYOZmVldfNWJmZmZFeNEw8zMzIpxomFmZmbFONEoTNKxkkZWuteQ9Is6YzIzM2sXJxrljQRukDRJ0m6ku35OrzkmMzOztvBVJ4VFxFGSLgduAB4H3hoRs2oOy8zMrC1co1FYfoz7D4FjgKuBH0naoNagzMzM2sQ1GuV9G3hfRMwEkPQe4Erg1bVGZWZm1gZONMp7Q0S81NURERdI8sPOzMxsueBEo7CIeEnSnsCWwKhKr2NqCsnMzKxt3EajMEknkR6k9mlAwPuAjWoNyszMrE2caJT3xoj4CPB4RHwVeAOwWc0xmZmZtYUTjfKey+/P5qtNXgTWrzEeMzOztnEbjfIuljQa+BZwE+nR8KfUG5KZmVl7ONEoLCKOzR9/I+liYFREPFlnTGZmZu3iRKMwSSOAPYEJ5OUtiYj4bp1xmZmZtYMTjfIuAv4J3A78q+ZYzMzM2sqJRnnjImJS3UGYmZnVwVedlHeJpN3rDsLMzKwOrtEo73rgt5JWIF3aKiAiYo16wzIzMyvPiUZ53yXdpOv2iIi6gzEzM2snnzopbw5wh5MMMzNbHrlGo7zZwNWSLgGe7yr05a1mZrY8cKJR3n35tVJ+Qbo7qJmZ2bDnRKO8mRHx62qBpPfVFYyZmVk7uY1GeUf1sszMzGzYcY1GIZLeCewBjJX0w0qvNYBF9URlZmbWXk40ylkAdAJTgOmV8oXA4bVEZGZm1mZONMr5aURsK+kdEXF63cGYmZnVwYlGOStJ+hCwg6T3NPaMiAtqiMnMzKytnGiUcwjwb8BoYK+GfgE40TAzs2HPiUYhEXEdcJ2kzoj4eV/HlzQZ+AEwAjglIr7Z0P8A4FvAg7noxxFxSu63IXAKMJ6U1OwREff3c1bMzMz6zYlGeb+UdBjw1tx9DXBSRLzYagRJI4ATgd2AucA0SVMjYmbDoOdGxKFNJnEG8PWIuEzSasC/lnkuzMzM+sH30SjvJ8B2+f0nwLbAT3sYZ3tgVkTMjogXgHOAvXvzZZK2AEZGxGUAEfF0RDzb3+DNzMyWhWs0ynt9RGxV6b5S0q09jDOW9DC2LnOBHZoM915JbwXuBg6PiDnAZsATki4ANgYuB46MiJf6PQdmZmb95BqN8l6S9KquDkmbAAPxo38RMCEiJgGXAV2X0I4E3gJ8Dng9sAlwQLMJSDpYUqekznnz5g1ASGZmZktyolHe54GrJF0t6RrgSuCIHsZ5kNSQs8s4Fjf6BCAi5kdE19NgTyGdnoFU+3FLPu2yCLiQdLpmKRFxckR0RETHmDFj+jRTZmZmveFTJ4VFxBWSJgKb56K7KglCK9OAiZI2JiUY+wEfqg4gaf2IeDh3TgHurIw7WtKYiJgH7Ey6Q6mZmVnbOdEoRNLrgTkR8UhEPC9pa+C9wAOSjo6IBa3GjYhFkg4FLiVd3npqRMyQdAzQGRFTgcMkTSE9N2UB+fRIRLwk6XPAFZJEuv35zwrOqpmZWUuKiLpjGJYk3QTsGhELcoPNc4BPA1sDr4mIfWsNsEFHR0d0drriw8wGlqTpEdFRdxxWH9dolDOiUmvxAeDkiPgN8BtJt9QYl5mZWdu4MWg5IyR1JXK7kBqBdnGCZ2ZmywX/4JVzNnCNpH8AzwF/ApC0KfBknYGZmZm1ixONQiLi65KuANYH/hiLG8OsQGqrYWZmNuw50SgoIq5vUnZ3HbGYmZnVwW00zMzMrBgnGmZmZlaMEw0zMzMrxomGmZmZFeNEw8zMzIpxomFmZmbFONEwMzOzYpxomJmZWTFONMzMzKwYJxpmZmZWjBMNMzMzK8aJhpmZmRXjRMPMzMyKcaJhZmZmxTjRMDMzs2KcaJiZmVkxTjTMzMysGCcaZmZmVowTDTMzMyvGiYaZmZkV40TDzMzMinGiYWZmZsU40TAzM7NinGiYmZlZMU40zMzMrBgnGmZmZlaMEw0zMzMrxomGmZmZFeNEY5CSNFnSXZJmSTqySf8DJM2TdEt+HVTp91KlfGp7IzczM1tsZN0B2NIkjQBOBHYD5gLTJE2NiJkNg54bEYc2mcRzEbF16TjNzMx64hqNwWl7YFZEzI6IF4BzgL1rjsnMzKzPnGgMTmOBOZXuubms0Xsl3SbpfEnjK+WjJHVKul7SPkUjNTMz64YTjaHrImBCREwCLgNOr/TbKCI6gA8B35f0qmYTkHRwTkg6582bVz5iMzNb7jjRGJweBKo1FONy2csiYn5EPJ87TwG2q/R7ML/PBq4Gtmn2JRFxckR0RETHmDFjBi56MzOzzInG4DQNmChpY0krAfsBS1w9Imn9SucU4M5cvpaklfPndYE3AY2NSM3MzNrCV50MQhGxSNKhwKXACODUiJgh6RigMyKmAodJmgIsAhYAB+TRXwP8r6R/kRLJbza5WsXMzKwtFBF1x2CDQEdHR3R2dtYdhpkNM5Km5zZjtpzyqRMzMzMrxomGmZmZFeNEw8zMzIpxomFmZmbFONEwMzOzYpxomJmZWTFONMzMzKwY30fDAJA0D3igj6OtC/yjQDjtNlzmAzwvg9XyPC8bRYSfcbAcc6Jh/SapczjciGe4zAd4XgYrz4stz3zqxMzMzIpxomFmZmbFONGwZXFy3QEMkOEyH+B5Gaw8L7bcchsNMzMzK8Y1GmZmZlaMEw3rM0mTJd0laZakI+uOpyeSxku6StJMSTMkfSaXry3pMkn35Pe1crkk/TDP322Stq13DpYkaYSkmyVdnLs3lnRDjvdcSSvl8pVz96zcf0KdcTeSNFrS+ZL+JulOSW8Ywuvk8Lxt3SHpbEmjhsp6kXSqpMck3VEp6/N6kLR/Hv4eSfvXMS82ODnRsD6RNAI4EXgnsAXwQUlb1BtVjxYBR0TEFsCOwKdyzEcCV0TEROCK3A1p3ibm18HAT9sfcrc+A9xZ6T4e+F5EbAo8Dnwsl38MeDyXfy8PN5j8APhDRLwa2Io0T0NunUgaCxwGdETEa4ERwH4MnfVyGjC5oaxP60HS2sBXgB2A7YGvdCUnZk40rK+2B2ZFxOyIeAE4B9i75pi6FREPR8RN+fNC0g/aWFLcp+fBTgf2yZ/3Bs6I5HpgtKT12xx2U5LGAXsCp+RuATsD5+dBGueja/7OB3bJw9dO0prAW4GfA0TECxHxBENwnWQjgVdIGgmsAjzMEFkvEXEtsKChuK/r4R3AZRGxICIeBy5j6eTFllNONKyvxgJzKt1zc9mQkKuptwFuANaLiIdzr0eA9fLnwTyP3we+APwrd68DPBERi3J3NdaX5yP3fzIPPxhsDMwDfpFPA50iaVWG4DqJiAeBbwN/JyUYTwLTGZrrpUtf18OgXT9WPycattyQtBrwG+A/I+Kpar9Il18N6kuwJL0LeCwiptcdywAYCWwL/DQitgGeYXH1PDA01glAPkWwNyl52gBYlWH0b36orAcbvJxoWF89CIyvdI/LZYOapBVJScaZEXFBLn60q/o9vz+WywfrPL4JmCLpftIpq51J7RxG5yp7WDLWl+cj918TmN/OgLsxF5gbETfk7vNJicdQWycAuwL3RcS8iHgRuIC0robieunS1/UwmNeP1cyJhvXVNGBiblG/EqnR29SaY+pWPv/9c+DOiPhupddUoKt1/P7A7yrlH8kt7HcEnqxUI9cmIo6KiHERMYG03K+MiH8DrgL2zYM1zkfX/O2bhx8U/0wj4hFgjqTNc9EuwEyG2DrJ/g7sKGmVvK11zcuQWy8VfV0PlwK7S1or1/DsnsvMICL88qtPL2AP4G7gXuDLdcfTi3jfTKr6vQ24Jb/2IJ0XvwK4B7gcWDsPL9KVNfcCt5OuJqh9PhrmaSfg4vx5E+BGYBbwa2DlXD4qd8/K/TepO+6Gedga6Mzr5UJgraG6ToCvAn8D7gB+Caw8VNYLcDapbcmLpJqmj/VnPQAH5nmaBXy07nXi1+B5+c6gZmZmVoxPnZiZmVkxTjTMzMysGCcaZmZmVowTDTMzMyvGiYaZmZkV40TDbAiRtJ6ksyTNljRd0l8lvXsZp3m0pM/lz8dI2rWf09la0h59GH60pE/257vMbOhwomE2ROSbQV0IXBsRm0TEdqQbd41rMuzIxrLeiIj/iYjL+xni1qT7k/TWaMCJhtkw50TDbOjYGXghIk7qKoiIByLiRwCSDpA0VdKVwBWSVpN0haSbJN0u6eWn7Er6sqS7JV0HbF4pP03SvvnzdpKuyTUnl1ZuSX21pOMl3Zin8ZZ8l9hjgA9IukXSB6qBS9oyD3+LpNskTQS+Cbwql30rD/d5SdPyMF/NZRMk/U3SmZLulHS+pFWKLGEzG3D9+tdjZrXYEriph2G2BSZFxIJcq/HuiHhK0rrA9ZKm5mH2I9VAjMzTXOJBbfnZMD8C9o6IeTlx+Drp7o8AIyNi+3yq5CsRsauk/yHdKfLQJnEdAvwgIs7MSckI0kPUXhsRW+fv3B2YCGxPugPlVElvJd3ie3PgYxHxZ0mnkmpCvt27xWZmdXKiYTZESTqRdHv1FyLi9bn4sohY0DUIcFz+sf4X6bHd6wFvAX4bEc/m6TR7Vs3mwGuBy9IZG0aQblPdpevBdNOBCb0I96/AlyWNAy6IiHvydKt2z6+bc/dqpMTj78CciPhzLv8VcBhONMyGBCcaZkPHDOC9XR0R8alcU9FZGeaZyud/A8YA20XEi/mpr6N6+V0CZkTEG1r0fz6/v0QvjiMRcZakG4A9gd9L+gQwu8l3fiMi/neJQmkCSz+m3M9OMBsi3EbDbOi4Ehgl6T8qZd21VVgTeCwnGW8HNsrl1wL7SHqFpNWBvZqMexcwRtIbIJ1KkbRlD/EtBFZv1kPSJsDsiPgh6Umgk5oMfylwoKTV8jhjJb0y99uwKxbgQ8B1PcRiZoOEEw2zISLSExD3Ad4m6T5JNwKnA19sMcqZQIek24GPkJ4uSkTcBJwL3ApcAkxr8l0vkB5hfrykW0lPvH1jDyFeBWzRrDEo8H7gDkm3kE7JnBER84E/S7pD0rci4o/AWcBfc8znszgRuQv4lKQ7SU95/WkPsZjZIOGnt5rZoJZPnVwcEa+tORQz6wfXaJiZmVkxrtEwMzOzYlyjYWZmZsU40TAzM7NinGiYmZlZMU40zMzMrBgnGmZmZlaMEw0zMzMr5v8DUPsEckpYlDkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4UJeJNsjvLZ"
      },
      "source": [
        "perturbed_patch_imgs = perturbed_patch_imgs.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}